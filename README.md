# Basic_Chat_bot
ğŸ¤– AI Chatbot with Llama 3, FastAPI & Streamlit
A full-stack AI-powered chatbot that uses Metaâ€™s Llama-3.3-70B-Instruct via the Together API, served through a FastAPI backend, and interacted with via a Streamlit frontend.

Perfect for experimenting with large language models locally or deploying as a lightweight AI assistant.

âœ¨ Features
LLM Integration: Uses meta-llama/Llama-3.3-70B-Instruct-Turbo-Free via Together AI.
FastAPI Backend: RESTful API endpoints for chat and LLM queries.
Streamlit UI: Simple, interactive web interface for chatting with the model.
Session History: Remembers conversation history during your session.
Scalable Design: Easy to extend with new models, endpoints, or UI components.

ğŸ“¦ Tech Stack
Backend: FastAPI (Python)
Frontend: Streamlit (Python)
LLM Provider: Together AI
Environment Management: python-dotenv
API Client: together Python SDK
ğŸš€ Getting Started

1. Prerequisites
Make sure you have:
Python 3.9+
pip or poetry
Together AI API key (set as TOGETHER_API_KEY in .env)
ğŸ‘‰ Get your free API key from Together AI
2. Clone & Install
bash
git clone https://github.com/yourusername/your-repo-name.git
cd your-repo-name

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install together python-dotenv fastapi uvicorn streamlit requests
git clone https://github.com/mahgoubhany/Basic_Chat_bot
cd your-repo-name

# Create virtual environment (optional but recommended)
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install together python-dotenv fastapi uvicorn streamlit requests
3. Setup Environment
Create a .env file in the root directory:

TOGETHER_API_KEY=your_together_api_key_here

4. Run the Backend (FastAPI)
uvicorn main:app --reload --host 0.0.0.0 --port 8000

API Docs: Visit http://localhost:8000/docs to explore endpoints. 

5. Run the Frontend (Streamlit)
In a new terminal:
bash
streamlit run main.py
Chat UI: Visit http://localhost:8501 

ğŸ§­ API Endpoints (FastAPI)
GET
/
Root endpoint â€” returns â€œhello worldâ€
GET
/hello/{name}
Greets user by name

POST
/chat/
Echoes user message (for testing)

POST
/llm/
Sends query to Llama 3 and returns AI response

Example /llm/ request body:

json
{
  "query": "What is the capital of France?"
}
ğŸ’¬ Example Chat
User: Best linux distro for daily use in brief
Bot: Ubuntu â€” user-friendly, great hardware support, large community, ideal for beginners and daily drivers.

ğŸ› ï¸ Project Structure

â”œâ”€â”€ main.py              # FastAPI + Streamlit (combined for simplicity)
â”œâ”€â”€ .env                 # Environment variables
â”œâ”€â”€ README.md            # You are here!
â””â”€â”€ requirements.txt     # Dependencies (optional)
ğŸ’¡ Note: In production, consider separating backend and frontend into different files/modules. 

ğŸ“„ License
MIT â€” Feel free to use, modify, and distribute.

ğŸ™Œ Acknowledgements
Together AI for free access to powerful LLMs
FastAPI for the blazing-fast backend
Streamlit for the effortless frontend
Llama 3 by Meta â€” the brain behind the bot
ğŸ“¬ Feedback & Contributions
Found a bug? Want to add a feature? Open an issue or submit a PR!


